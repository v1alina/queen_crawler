{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb675bdd-39c8-436f-a10c-76e69f3a4abd",
   "metadata": {},
   "source": [
    "# Queen's Christmas Broadcast crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff72f3e-8e42-4a88-b4c2-18df3c92684c",
   "metadata": {
    "tags": []
   },
   "source": [
    "This webcrawler is created for the purpose of corpus creation of the Queen Elizabeth's Christmas broadcasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276cf6e8-8750-4651-81b6-525324aa1bb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be969e-3136-49c1-b88a-6868685f292c",
   "metadata": {},
   "source": [
    "First off, there are some libraries we need for buidling the crawler.\n",
    "\n",
    "Requests is for making internet requests to servers\n",
    "Beautiful Soup is for filtering through HTML documents\n",
    "the join function is useful when we want to create filepaths that work for all possible OS (Windows and MacOS use different ways of creating filepaths)\n",
    "makedirs is so that we can create a folder where to save our html and later corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d2b6dd6-da4c-4422-bfd2-c4278b460bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from os import makedirs\n",
    "from os.path import join\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2b63c-0ba8-471c-bcc9-52bf23a76545",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set-up the crawler:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ddcd2c-8662-4f9e-b772-4c791785246a",
   "metadata": {},
   "source": [
    "When setting up a web-crawler we want to look at the structure of the web page we want to extract the HTML from. In our case, we have a nice page that links to all of the transcripts: https://www.royal.uk/history-christmas-broadcast One option is to use this page extract all the links and then download the pages with the links we sourced.\n",
    "\n",
    "However, we could also see if there is a pattern in the URLs that helps us download the transcripts directely without having to extract the ahrf/links with Beautiful Soup. This could save us some time and would make the process a bit more clean as we do not need to save an index file for these links.\n",
    "\n",
    "So let's check if there is a pattern in the URLs, which we could use to build the web crawler. Have a look at the following links:\n",
    "\n",
    "- https://www.royal.uk/queens-first-christmas-broadcast-1952\n",
    "- https://www.royal.uk/christmas-broadcast-1954\n",
    "- https://www.royal.uk/christmas-broadcast-1962\n",
    "- https://www.royal.uk/christmas-broadcast-2006\n",
    "- https://www.royal.uk/christmas-broadcast-2021\n",
    "\n",
    "We can see that there is a pattern but there is also annoyingly one outlier: the very first Christmas broadcast has a different URL than all other broadcasts. While this is a bit of a nuisance, it is still something we can consider in our design process and adjust to, afterall all the other speech seem to follow a pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b2e1a-ccec-4631-9365-c910e6c96d5d",
   "metadata": {},
   "source": [
    "First, let's set up the constant variables we will need later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39681ad1-e0d2-4128-83a8-fbf66483a118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Setting up constant vars\n",
    "makedirs('./transcript-pages', exist_ok=True)\n",
    "TRANS_PAGES_DIR = \"transcript-pages\"\n",
    "\n",
    "\n",
    "THE_URL = 'https://www.royal.uk/christmas-broadcast-' #this is the base of the URL that we can reuse in loop later\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97297b-4dba-4a70-8e59-f9f3541a3977",
   "metadata": {
    "tags": []
   },
   "source": [
    "You might be wondering what the *header* variable is for. I am using this because the royal website blocks requests coming from scripts directly. This header makes it seem as if the request is coming from a browser and not from a python script. Here is the status code we get when we make a request without changing the header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcba3aad-7e29-4da1-b3f1-94266c4ba2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    }
   ],
   "source": [
    "example_year = 1998\n",
    "url = THE_URL + str(example_year)\n",
    "resp = requests.get(url)\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1131d77-190b-49c3-8e5f-b7d9b0a67bd2",
   "metadata": {},
   "source": [
    "**Status code 403** means that our request got blocked by the server, so let's try it again with using our header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "524fa038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get(url, headers = headers)\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11556f7-eb11-4395-a83a-c91c1e1d0cb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we get **status code 200**, which means our request is successful and we got a response from the server. With this being resolved, we can move towards saving the HTML files of the pages. Just to be extra sure, I want to check the status of each page before downloading and saving it. Otherwise, we might end up saving an empty file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94dfeab4-0d28-4961-a552-2e35312c5d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.royal.uk/queens-first-christmas-broadcast-1952\n",
      "Saving as transcript-pages/1952.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1953\n",
      "Saving as transcript-pages/1953.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1954\n",
      "Saving as transcript-pages/1954.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1955\n",
      "Saving as transcript-pages/1955.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1956\n",
      "Saving as transcript-pages/1956.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1957\n",
      "Saving as transcript-pages/1957.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1958\n",
      "Saving as transcript-pages/1958.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1959\n",
      "Saving as transcript-pages/1959.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1960\n",
      "Saving as transcript-pages/1960.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1961\n",
      "Saving as transcript-pages/1961.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1962\n",
      "Saving as transcript-pages/1962.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1963\n",
      "Saving as transcript-pages/1963.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1964\n",
      "Saving as transcript-pages/1964.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1965\n",
      "Saving as transcript-pages/1965.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1966\n",
      "Saving as transcript-pages/1966.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1967\n",
      "Saving as transcript-pages/1967.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1968\n",
      "Saving as transcript-pages/1968.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1969\n",
      "Saving as transcript-pages/1969.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1970\n",
      "Saving as transcript-pages/1970.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1971\n",
      "Saving as transcript-pages/1971.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1972\n",
      "Saving as transcript-pages/1972.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1973\n",
      "Saving as transcript-pages/1973.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1974\n",
      "Saving as transcript-pages/1974.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1975\n",
      "Saving as transcript-pages/1975.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1976\n",
      "Saving as transcript-pages/1976.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1977\n",
      "Saving as transcript-pages/1977.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1978\n",
      "Saving as transcript-pages/1978.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1979\n",
      "Saving as transcript-pages/1979.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1980\n",
      "Saving as transcript-pages/1980.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1981\n",
      "Saving as transcript-pages/1981.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1982\n",
      "Saving as transcript-pages/1982.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1983\n",
      "Saving as transcript-pages/1983.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1984\n",
      "Saving as transcript-pages/1984.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1985\n",
      "Saving as transcript-pages/1985.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1986\n",
      "Saving as transcript-pages/1986.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1987\n",
      "Saving as transcript-pages/1987.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1988\n",
      "Saving as transcript-pages/1988.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1989\n",
      "Saving as transcript-pages/1989.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1990\n",
      "Saving as transcript-pages/1990.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1991\n",
      "Saving as transcript-pages/1991.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1992\n",
      "Saving as transcript-pages/1992.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1993\n",
      "Saving as transcript-pages/1993.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1994\n",
      "Saving as transcript-pages/1994.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1995\n",
      "Saving as transcript-pages/1995.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1996\n",
      "Saving as transcript-pages/1996.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1997\n",
      "Saving as transcript-pages/1997.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1998\n",
      "Saving as transcript-pages/1998.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-1999\n",
      "Saving as transcript-pages/1999.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2000\n",
      "Saving as transcript-pages/2000.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2001\n",
      "Saving as transcript-pages/2001.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2002\n",
      "Saving as transcript-pages/2002.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2003\n",
      "Saving as transcript-pages/2003.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2004\n",
      "Saving as transcript-pages/2004.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2005\n",
      "Saving as transcript-pages/2005.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2006\n",
      "Saving as transcript-pages/2006.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2007\n",
      "Saving as transcript-pages/2007.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2008\n",
      "Saving as transcript-pages/2008.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2009\n",
      "Saving as transcript-pages/2009.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2010\n",
      "Saving as transcript-pages/2010.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2011\n",
      "Saving as transcript-pages/2011.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2012\n",
      "Saving as transcript-pages/2012.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2013\n",
      "Saving as transcript-pages/2013.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2014\n",
      "Saving as transcript-pages/2014.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2015\n",
      "Saving as transcript-pages/2015.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2016\n",
      "Saving as transcript-pages/2016.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2017\n",
      "Saving as transcript-pages/2017.html\n",
      "Request for https://www.royal.uk/christmas-broadcast-2018 was not successful.\n",
      "Not saving file.\n",
      "Request for https://www.royal.uk/christmas-broadcast-2019 was not successful.\n",
      "Not saving file.\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2020\n",
      "Saving as transcript-pages/2020.html\n",
      "Downloading https://www.royal.uk/christmas-broadcast-2021\n",
      "Saving as transcript-pages/2021.html\n",
      "Request for https://www.royal.uk/christmas-broadcast-2022 was not successful.\n",
      "Not saving file.\n"
     ]
    }
   ],
   "source": [
    "# This is for downloading the one URL that differs from the usual pattern:\n",
    "\n",
    "exception_url = 'https://www.royal.uk/queens-first-christmas-broadcast-1952'\n",
    "resp = requests.get(exception_url, headers = headers)\n",
    "if resp.status_code == 200:\n",
    "        print(\"Downloading\", resp.url)\n",
    "        fname = join(TRANS_PAGES_DIR, '{}.html'.format(1952))\n",
    "        with open(fname, \"w\") as wf:\n",
    "            print(\"Saving as\", fname)\n",
    "            wf.write(resp.text)\n",
    "else:\n",
    "    print(\"Request for\", resp.url, \"was not successful.\\nNot saving file.\")\n",
    "\n",
    "# Now we can download all of the other HTML files for which the URLs increment by year.\n",
    "# For this I am simply using a for loop to go through the years.\n",
    "\n",
    "start_year = 1953\n",
    "end_year = 2021 + 1 # Adding an extra year because otherwise it will only download up until 2021\n",
    "\n",
    "for page_num in range(start_year, end_year): \n",
    "    url = THE_URL + str(page_num)\n",
    "    resp = requests.get(url, headers = headers)\n",
    "    if resp.status_code == 200:\n",
    "        print(\"Downloading\", resp.url)\n",
    "        fname = join(TRANS_PAGES_DIR, '{}.html'.format(page_num))\n",
    "        with open(fname, \"w\") as wf:\n",
    "            print(\"Saving as\", fname)\n",
    "            wf.write(resp.text)\n",
    "    else:\n",
    "        print(\"Request for\", resp.url, \"was not successful.\\nNot saving file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97702024-3b38-4203-95e3-d347e52c4fef",
   "metadata": {},
   "source": [
    "Oh, no. It seems that there might be some more outlier URLs...\n",
    "2018 and 2019 were not successful requests and it seems these pages do not exist even though there were speeches given by the queen. Let's have a look at the URLs:\n",
    "- https://www.royal.uk/queens-christmas-broadcast-2018\n",
    "- https://www.royal.uk/queen’s-christmas-broadcast-2019\n",
    "\n",
    "We can see that these followed a different naming convention than most other speeches. What this shows us is that using URL patterns is inconsistent as URL naming can vary from time to time. This would not be the case if there were page queries used. Consider for instance this URL from the White House Press Briefing:\n",
    "- https://www.whitehouse.gov/briefing-room/?page=0\n",
    "\n",
    "The **?page=0** is a so-called *query string*, which can be used at the end of an URL to query a wide number of things such as the page number. If the transcripts were organized in such a way that we could have use a query string, we wouldn't have to make any exceptions. This shows that just using URL patterns can lead to inconsistencies and when we are presented with this, it might be better to scrape the links using the [index page](https://www.royal.uk/history-christmas-broadcast) and Beautiful Soup.  Funnily enough however, my group mate asked me if I was able to get the broadcast for 2017, which was successful. However, the link given on the [index webpage](https://www.royal.uk/history-christmas-broadcast) seems to be wrong and leads to an non-existing page. \n",
    "\n",
    "Ironically, this means that the crawler I built was able to get the correct link when the wrong link was given on the actual index-page. The conclusion I draw from this is that one way or another, we can encounter incostiencies when webscraping so there is not one way to go about and do it. It's important that we make sure to check the status of our requests to identify problems early on and not later on when cleaning the corpus.\n",
    "\n",
    "Let's just scrape the last two broadcasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1733d532-b84a-4fe5-8684-5c906a93c619",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.royal.uk/queens-christmas-broadcast-2018\n",
      "Saving as transcript-pages/2018.html\n",
      "Downloading https://www.royal.uk/queen%E2%80%99s-christmas-broadcast-2019\n",
      "Saving as transcript-pages/2019.html\n"
     ]
    }
   ],
   "source": [
    "# Broadcast from 2018\n",
    "\n",
    "exception_url_2018 = 'https://www.royal.uk/queens-christmas-broadcast-2018'\n",
    "resp = requests.get(exception_url_2018, headers = headers)\n",
    "if resp.status_code == 200:\n",
    "        print(\"Downloading\", resp.url)\n",
    "        fname = join(TRANS_PAGES_DIR, '{}.html'.format(2018)) #add correct year manually\n",
    "            print(\"Saving as\", fname)\n",
    "            wf.write(resp.text)\n",
    "else:\n",
    "    print(\"Request for\", resp.url, \"was not successful.\\nNot saving file.\")\n",
    "\n",
    "    \n",
    "# Broadcastfrom 2019\n",
    "\n",
    "exception_url_2019 = 'https://www.royal.uk/queen’s-christmas-broadcast-2019' #Correct link here\n",
    "resp = requests.get(exception_url_2019, headers = headers)\n",
    "if resp.status_code == 200:\n",
    "        print(\"Downloading\", resp.url)\n",
    "        fname = join(TRANS_PAGES_DIR, '{}.html'.format(2019)) #add correct year manually\n",
    "        with open(fname, \"w\") as wf:\n",
    "            print(\"Saving as\", fname)\n",
    "            wf.write(resp.text)\n",
    "else:\n",
    "    print(\"Request for\", resp.url, \"was not successful.\\nNot saving file.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebcf0bc-1c29-4993-b368-ade24ad71aa4",
   "metadata": {},
   "source": [
    "Now we should finally have all the necessary HTML files and can move to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a9b56-0ce2-44b7-af34-4f49411d62c0",
   "metadata": {},
   "source": [
    "## Creating txt files from the HTML files using Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5ff19-2a91-4a62-aede-41e257578ba6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Beautiful Soup example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370f52b7-880c-474c-bd6e-3685ed2d2399",
   "metadata": {},
   "source": [
    "To demonstrate the basic use of the parser let's have a look at how it works with an example first. I am creating an example, which I took from the [documentation page of the Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd0d3bd3-d8d6-4c24-936d-8a1029ce33a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_example = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html_example, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b15cc4-8a19-4183-8c91-a91a5a6db555",
   "metadata": {
    "tags": []
   },
   "source": [
    "This creates a Beautiful soup object object that we can do a bunch of different things with later on. Note, that we also have to pass a so-called *parser* when we create the object. I used 'lxml' because I have seen other people use it and it seems like a decent often-used parser. Let's try out some of the things we can do with this object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8175bd75-3850-4207-9eb7-5b349ba0bd80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[document]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56065f4d-00da-4723-9de4-6109436a69ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head><title>The Dormouse's story</title></head>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95c17bc7-91dc-4d06-a2dc-06fccc4017ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"title\"><b>The Dormouse's story</b></p>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27fe5dbb-6acf-4d6a-badf-2e4a9969ef66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\\n\\nThe Dormouse's story\\nOnce upon a time there were three little sisters; and their names were\\nElsie,\\nLacie and\\nTillie;\\nand they lived at the bottom of a well.\\n...\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b725322-f622-4429-af8b-d67568e62500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<html><head><title>The Dormouse's story</title></head>\n",
       " <body>\n",
       " <p class=\"title\"><b>The Dormouse's story</b></p>\n",
       " <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>\n",
       " <p class=\"story\">...</p>\n",
       " </body></html>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3063dae9-ded3-4b1e-9c7f-91d8c9a63b29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"title\"><b>The Dormouse's story</b></p>,\n",
       " <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>,\n",
       " <p class=\"story\">...</p>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047500ed-12d0-452b-afab-24e64d7c4da1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's start unraveling our HTML docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf388986-bd64-46ef-b565-ea46a58b6b28",
   "metadata": {},
   "source": [
    "---\n",
    "The find_all method above is one of the commonly used methods for searching your HTML document.\n",
    "Let's see what happens when we open up one of our texts and try to find all the 'p' (paragraphs) elements in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b8f50f9c-7633-4449-a007-eb1c1aa5b8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_year = 1968\n",
    "end_year = 1969\n",
    "\n",
    "for page_num in range(start_year, end_year):\n",
    "    broadcast = join(TRANS_PAGES_DIR, '{}.html'.format(page_num))\n",
    "    with open(broadcast, \"r\") as file:\n",
    "        broadcast1968 = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "broadcast1968.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a1d96da-cf95-41d5-bbd0-bd21b7d8b8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_year = 2019\n",
    "end_year = 2020\n",
    "\n",
    "for page_num in range(start_year, end_year):\n",
    "    broadcast = join(TRANS_PAGES_DIR, '{}.html'.format(page_num))\n",
    "    with open(broadcast, \"r\") as file:\n",
    "        broadcast2019 = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "broadcast2019.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e29f0e-63c2-44c2-b02c-db8493961428",
   "metadata": {},
   "source": [
    "This is not a bad start. \n",
    "\n",
    "We do seem to be getting the entirety of the speech, but there are also some unnessary things we might need to filter out such as the footer and the \"Share this article\" p element. Additionally, we can see that the formatting in these articles is consistent again. \n",
    "\n",
    "While in the [2019 broadcast](https://www.royal.uk/queen’s-christmas-broadcast-2019) there is a \"share this article\" p element, the [1968 broadcast](https://www.royal.uk/christmas-broadcast-1968) also has this but lacks the little description and framed quote at the beginning. Unfortunately, there are no class attributes it seems that highlight this clearly but the description is encapsulated in an \\<em> element('em' standing for *emphasis*). However, when looking through the webpages, I have also seen description that do not have any special formatting and are not in cursive. Therefore, I unfortunately cannot rely on the em-element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3a975702-0028-4b5b-8880-fac35a9044bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "merged = \"\"\n",
    "for paragraph in broadcast1968.find_all('p'):\n",
    "    paragraphs.append(paragraph.text)\n",
    "\n",
    "merged = \"\\n\".join(paragraphs) #I am using the line break to preserve the more orginal structure of the written broadcast\n",
    "\n",
    "#print(merged) #For checking if eveything saved correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c807f485-3243-4b81-ad7a-6bbbd09dae94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I was debating if I want to split the text but I decided against it for now.\n",
    "\n",
    "#end_phrase = 'share this article'\n",
    "#if end_phrase in merged.lower():\n",
    "#    print('yey')\n",
    "#   print(merged.lower().split(end_phrase)) #we are splitting the text at the end phrase once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59377d2d-413e-400c-903c-355d25d13694",
   "metadata": {},
   "source": [
    "Now it's time to bring those two chunks of code together into one loop that goes through the HTML files and saves the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "afaf723c-3b30-4f1d-b1d1-152e0f54a129",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening and converting transcript-pages/1952.html\n",
      "Saving as corpus/1952.txt\n",
      "Opening and converting transcript-pages/1953.html\n",
      "Saving as corpus/1953.txt\n",
      "Opening and converting transcript-pages/1954.html\n",
      "Saving as corpus/1954.txt\n",
      "Opening and converting transcript-pages/1955.html\n",
      "Saving as corpus/1955.txt\n",
      "Opening and converting transcript-pages/1956.html\n",
      "Saving as corpus/1956.txt\n",
      "Opening and converting transcript-pages/1957.html\n",
      "Saving as corpus/1957.txt\n",
      "Opening and converting transcript-pages/1958.html\n",
      "Saving as corpus/1958.txt\n",
      "Opening and converting transcript-pages/1959.html\n",
      "Saving as corpus/1959.txt\n",
      "Opening and converting transcript-pages/1960.html\n",
      "Saving as corpus/1960.txt\n",
      "Opening and converting transcript-pages/1961.html\n",
      "Saving as corpus/1961.txt\n",
      "Opening and converting transcript-pages/1962.html\n",
      "Saving as corpus/1962.txt\n",
      "Opening and converting transcript-pages/1963.html\n",
      "Saving as corpus/1963.txt\n",
      "Opening and converting transcript-pages/1964.html\n",
      "Saving as corpus/1964.txt\n",
      "Opening and converting transcript-pages/1965.html\n",
      "Saving as corpus/1965.txt\n",
      "Opening and converting transcript-pages/1966.html\n",
      "Saving as corpus/1966.txt\n",
      "Opening and converting transcript-pages/1967.html\n",
      "Saving as corpus/1967.txt\n",
      "Opening and converting transcript-pages/1968.html\n",
      "Saving as corpus/1968.txt\n",
      "Opening and converting transcript-pages/1969.html\n",
      "Saving as corpus/1969.txt\n",
      "Opening and converting transcript-pages/1970.html\n",
      "Saving as corpus/1970.txt\n",
      "Opening and converting transcript-pages/1971.html\n",
      "Saving as corpus/1971.txt\n",
      "Opening and converting transcript-pages/1972.html\n",
      "Saving as corpus/1972.txt\n",
      "Opening and converting transcript-pages/1973.html\n",
      "Saving as corpus/1973.txt\n",
      "Opening and converting transcript-pages/1974.html\n",
      "Saving as corpus/1974.txt\n",
      "Opening and converting transcript-pages/1975.html\n",
      "Saving as corpus/1975.txt\n",
      "Opening and converting transcript-pages/1976.html\n",
      "Saving as corpus/1976.txt\n",
      "Opening and converting transcript-pages/1977.html\n",
      "Saving as corpus/1977.txt\n",
      "Opening and converting transcript-pages/1978.html\n",
      "Saving as corpus/1978.txt\n",
      "Opening and converting transcript-pages/1979.html\n",
      "Saving as corpus/1979.txt\n",
      "Opening and converting transcript-pages/1980.html\n",
      "Saving as corpus/1980.txt\n",
      "Opening and converting transcript-pages/1981.html\n",
      "Saving as corpus/1981.txt\n",
      "Opening and converting transcript-pages/1982.html\n",
      "Saving as corpus/1982.txt\n",
      "Opening and converting transcript-pages/1983.html\n",
      "Saving as corpus/1983.txt\n",
      "Opening and converting transcript-pages/1984.html\n",
      "Saving as corpus/1984.txt\n",
      "Opening and converting transcript-pages/1985.html\n",
      "Saving as corpus/1985.txt\n",
      "Opening and converting transcript-pages/1986.html\n",
      "Saving as corpus/1986.txt\n",
      "Opening and converting transcript-pages/1987.html\n",
      "Saving as corpus/1987.txt\n",
      "Opening and converting transcript-pages/1988.html\n",
      "Saving as corpus/1988.txt\n",
      "Opening and converting transcript-pages/1989.html\n",
      "Saving as corpus/1989.txt\n",
      "Opening and converting transcript-pages/1990.html\n",
      "Saving as corpus/1990.txt\n",
      "Opening and converting transcript-pages/1991.html\n",
      "Saving as corpus/1991.txt\n",
      "Opening and converting transcript-pages/1992.html\n",
      "Saving as corpus/1992.txt\n",
      "Opening and converting transcript-pages/1993.html\n",
      "Saving as corpus/1993.txt\n",
      "Opening and converting transcript-pages/1994.html\n",
      "Saving as corpus/1994.txt\n",
      "Opening and converting transcript-pages/1995.html\n",
      "Saving as corpus/1995.txt\n",
      "Opening and converting transcript-pages/1996.html\n",
      "Saving as corpus/1996.txt\n",
      "Opening and converting transcript-pages/1997.html\n",
      "Saving as corpus/1997.txt\n",
      "Opening and converting transcript-pages/1998.html\n",
      "Saving as corpus/1998.txt\n",
      "Opening and converting transcript-pages/1999.html\n",
      "Saving as corpus/1999.txt\n",
      "Opening and converting transcript-pages/2000.html\n",
      "Saving as corpus/2000.txt\n",
      "Opening and converting transcript-pages/2001.html\n",
      "Saving as corpus/2001.txt\n",
      "Opening and converting transcript-pages/2002.html\n",
      "Saving as corpus/2002.txt\n",
      "Opening and converting transcript-pages/2003.html\n",
      "Saving as corpus/2003.txt\n",
      "Opening and converting transcript-pages/2004.html\n",
      "Saving as corpus/2004.txt\n",
      "Opening and converting transcript-pages/2005.html\n",
      "Saving as corpus/2005.txt\n",
      "Opening and converting transcript-pages/2006.html\n",
      "Saving as corpus/2006.txt\n",
      "Opening and converting transcript-pages/2007.html\n",
      "Saving as corpus/2007.txt\n",
      "Opening and converting transcript-pages/2008.html\n",
      "Saving as corpus/2008.txt\n",
      "Opening and converting transcript-pages/2009.html\n",
      "Saving as corpus/2009.txt\n",
      "Opening and converting transcript-pages/2010.html\n",
      "Saving as corpus/2010.txt\n",
      "Opening and converting transcript-pages/2011.html\n",
      "Saving as corpus/2011.txt\n",
      "Opening and converting transcript-pages/2012.html\n",
      "Saving as corpus/2012.txt\n",
      "Opening and converting transcript-pages/2013.html\n",
      "Saving as corpus/2013.txt\n",
      "Opening and converting transcript-pages/2014.html\n",
      "Saving as corpus/2014.txt\n",
      "Opening and converting transcript-pages/2015.html\n",
      "Saving as corpus/2015.txt\n",
      "Opening and converting transcript-pages/2016.html\n",
      "Saving as corpus/2016.txt\n",
      "Opening and converting transcript-pages/2017.html\n",
      "Saving as corpus/2017.txt\n",
      "Opening and converting transcript-pages/2018.html\n",
      "Saving as corpus/2018.txt\n",
      "Opening and converting transcript-pages/2019.html\n",
      "Saving as corpus/2019.txt\n",
      "Opening and converting transcript-pages/2020.html\n",
      "Saving as corpus/2020.txt\n",
      "Opening and converting transcript-pages/2021.html\n",
      "Saving as corpus/2021.txt\n"
     ]
    }
   ],
   "source": [
    "makedirs('./corpus', exist_ok=True)\n",
    "path = \"corpus\"\n",
    "\n",
    "start_year = 1952\n",
    "end_year = 2021 +1\n",
    "\n",
    "\n",
    "for year in range(start_year, end_year):\n",
    "    \n",
    "    broadcast = join(TRANS_PAGES_DIR, '{}.html'.format(year)) \n",
    "    with open(broadcast, \"r\") as file:\n",
    "        print(\"Opening and converting\", broadcast)\n",
    "        file = BeautifulSoup(file, 'lxml')\n",
    "        \n",
    "    paragraphs = []\n",
    "    merged = \"\"\n",
    "    file_path = join(path, '{}.txt'.format(year)) \n",
    "    \n",
    "    for paragraph in file.find_all('p'):\n",
    "        paragraphs.append(paragraph.text)\n",
    "        merged = \"\\n\".join(paragraphs)\n",
    "    \n",
    "    with open(file_path, \"w\") as file:\n",
    "        print(\"Saving as\", file_path)\n",
    "        file.write(merged)\n",
    "         #   print(\"Saving as\", year)\n",
    "         #   wf.write(resp.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
